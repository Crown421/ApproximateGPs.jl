<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · ApproximateGPs.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ApproximateGPs.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../userguide/">User Guide</a></li><li class="is-active"><a class="tocitem" href>API</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/a-regression/">Regression: Sparse Variational Gaussian Process for Stochastic Optimisation with Flux.jl</a></li><li><a class="tocitem" href="../examples/b-classification/">Classification: Sparse Variational Approximation for Non-Conjugate Likelihoods with Optim&#39;s L-BFGS</a></li><li><a class="tocitem" href="../examples/c-comparisons/">Binary Classification with Laplace approximation</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ApproximateGPs-API"><a class="docs-heading-anchor" href="#ApproximateGPs-API">ApproximateGPs API</a><a id="ApproximateGPs-API-1"></a><a class="docs-heading-anchor-permalink" href="#ApproximateGPs-API" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.SparseVariationalApproximation" href="#ApproximateGPs.SparseVariationalApproximation"><code>ApproximateGPs.SparseVariationalApproximation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SparseVariationalApproximation(fz::FiniteGP, q::AbstractMvNormal)</code></pre><p>Packages the prior over the pseudo-points, <code>fz</code>, and the approximate posterior at the pseudo-points, <code>q</code>, together into a single object.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/sparse_variational.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.FiniteGP{var&quot;#s31&quot;, var&quot;#s32&quot;, var&quot;#s33&quot;} where {var&quot;#s31&quot;&lt;:AbstractGPs.AbstractGP, var&quot;#s32&quot;&lt;:(AbstractVector{T} where T), var&quot;#s33&quot;&lt;:(LinearAlgebra.Diagonal{var&quot;#s34&quot;, var&quot;#s35&quot;} where {var&quot;#s34&quot;&lt;:Real, var&quot;#s35&quot;&lt;:FillArrays.Fill})}, AbstractVector{var&quot;#s36&quot;} where var&quot;#s36&quot;&lt;:Real}" href="#AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.FiniteGP{var&quot;#s31&quot;, var&quot;#s32&quot;, var&quot;#s33&quot;} where {var&quot;#s31&quot;&lt;:AbstractGPs.AbstractGP, var&quot;#s32&quot;&lt;:(AbstractVector{T} where T), var&quot;#s33&quot;&lt;:(LinearAlgebra.Diagonal{var&quot;#s34&quot;, var&quot;#s35&quot;} where {var&quot;#s34&quot;&lt;:Real, var&quot;#s35&quot;&lt;:FillArrays.Fill})}, AbstractVector{var&quot;#s36&quot;} where var&quot;#s36&quot;&lt;:Real}"><code>AbstractGPs.elbo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">elbo(svgp::SparseVariationalApproximation, fx::FiniteGP, y::AbstractVector{&lt;:Real}; num_data=length(y), quadrature=DefaultQuadrature())</code></pre><p>Compute the Evidence Lower BOund from [1] for the process <code>f = fx.f == svgp.fz.f</code> where <code>y</code> are observations of <code>fx</code>, pseudo-inputs are given by <code>z = svgp.fz.x</code> and <code>q(u)</code> is a variational distribution over inducing points <code>u = f(z)</code>.</p><p><code>quadrature</code> selects which method is used to calculate the expected loglikelihood in the ELBO. The options are: <code>DefaultQuadrature()</code>, <code>Analytic()</code>, <code>GaussHermite()</code> and <code>MonteCarlo()</code>. For likelihoods with an analytic solution, <code>DefaultQuadrature()</code> uses this exact solution. If there is no such solution, <code>DefaultQuadrature()</code> either uses <code>GaussHermite()</code> or <code>MonteCarlo()</code>, depending on the likelihood.</p><p>N.B. the likelihood is assumed to be Gaussian with observation noise <code>fx.Σy</code>. Further, <code>fx.Σy</code> must be isotropic - i.e. <code>fx.Σy = α * I</code>.</p><p>[1] - Hensman, James, Alexander Matthews, and Zoubin Ghahramani. &quot;Scalable variational Gaussian process classification.&quot; Artificial Intelligence and Statistics. PMLR, 2015.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/elbo.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.LatentFiniteGP, AbstractVector{T} where T}" href="#AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.LatentFiniteGP, AbstractVector{T} where T}"><code>AbstractGPs.elbo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">elbo(svgp, ::SparseVariationalApproximation, lfx::LatentFiniteGP, y::AbstractVector; num_data=length(y), quadrature=DefaultQuadrature())</code></pre><p>Compute the ELBO for a LatentGP with a possibly non-conjugate likelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/elbo.jl#L41-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="AbstractGPs.posterior-Tuple{LaplaceApproximation, AbstractGPs.LatentFiniteGP, Any}" href="#AbstractGPs.posterior-Tuple{LaplaceApproximation, AbstractGPs.LatentFiniteGP, Any}"><code>AbstractGPs.posterior</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">posterior(la::LaplaceApproximation, lfx::LatentFiniteGP, ys)</code></pre><p>Construct a Gaussian approximation <code>q(f)</code> to the posterior <code>p(f | y)</code> using the Laplace approximation. Solves for a mode of the posterior using Newton&#39;s method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L13-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.approx_lml-Tuple{LaplaceApproximation, AbstractGPs.LatentFiniteGP, Any}" href="#ApproximateGPs.approx_lml-Tuple{LaplaceApproximation, AbstractGPs.LatentFiniteGP, Any}"><code>ApproximateGPs.approx_lml</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">approx_lml(la::LaplaceApproximation, lfx::LatentFiniteGP, ys)</code></pre><p>Compute an approximation to the log of the marginal likelihood (also known as &quot;evidence&quot;), which can be used to optimise the hyperparameters of <code>lfx</code>.</p><p>This should become part of the AbstractGPs API (see JuliaGaussianProcesses/AbstractGPs.jl#221).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L31-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.build_laplace_objective-Tuple{Any, Any, Any}" href="#ApproximateGPs.build_laplace_objective-Tuple{Any, Any, Any}"><code>ApproximateGPs.build_laplace_objective</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">build_laplace_objective(build_latent_gp, xs, ys; kwargs...)</code></pre><p>Construct a closure that computes the minimisation objective for optimising hyperparameters of the latent GP in the Laplace approximation. The returned closure passes its arguments to <code>build_latent_gp</code>, which must return the <code>LatentGP</code> prior.</p><p><strong>Keyword arguments</strong></p><ul><li><code>newton_warmstart=true</code>: (default) begin Newton optimisation at the mode of the previous call to the objective</li><li><code>newton_callback</code>: called as <code>newton_callback(fnew, cache)</code> after each Newton step</li><li><code>newton_maxiter=100</code>: maximum number of Newton steps.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L43-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.expected_loglik-NTuple{4, Any}" href="#ApproximateGPs.expected_loglik-NTuple{4, Any}"><code>ApproximateGPs.expected_loglik</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">expected_loglik(quadrature::QuadratureMethod, y::AbstractVector, q_f::AbstractVector{&lt;:Normal}, lik)</code></pre><p>This function computes the expected log likelihood:</p><p class="math-container">\[    ∫ q(f) log p(y | f) df\]</p><p>where <code>p(y | f)</code> is the process likelihood. This is described by <code>lik</code>, which should be a callable that takes <code>f</code> as input and returns a Distribution over <code>y</code> that supports <code>loglikelihood(lik(f), y)</code>.</p><p><code>q(f)</code> is an approximation to the latent function values <code>f</code> given by:</p><p class="math-container">\[    q(f) = ∫ p(f | u) q(u) du\]</p><p>where <code>q(u)</code> is the variational distribution over inducing points (see <a href="#AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.FiniteGP{var&quot;#s31&quot;, var&quot;#s32&quot;, var&quot;#s33&quot;} where {var&quot;#s31&quot;&lt;:AbstractGPs.AbstractGP, var&quot;#s32&quot;&lt;:(AbstractVector{T} where T), var&quot;#s33&quot;&lt;:(LinearAlgebra.Diagonal{var&quot;#s34&quot;, var&quot;#s35&quot;} where {var&quot;#s34&quot;&lt;:Real, var&quot;#s35&quot;&lt;:FillArrays.Fill})}, AbstractVector{var&quot;#s36&quot;} where var&quot;#s36&quot;&lt;:Real}"><code>elbo</code></a>). The marginal distributions of <code>q(f)</code> are given by <code>q_f</code>.</p><p><code>quadrature</code> determines which method is used to calculate the expected log likelihood - see <a href="#AbstractGPs.elbo-Tuple{SparseVariationalApproximation, AbstractGPs.FiniteGP{var&quot;#s31&quot;, var&quot;#s32&quot;, var&quot;#s33&quot;} where {var&quot;#s31&quot;&lt;:AbstractGPs.AbstractGP, var&quot;#s32&quot;&lt;:(AbstractVector{T} where T), var&quot;#s33&quot;&lt;:(LinearAlgebra.Diagonal{var&quot;#s34&quot;, var&quot;#s35&quot;} where {var&quot;#s34&quot;&lt;:Real, var&quot;#s35&quot;&lt;:FillArrays.Fill})}, AbstractVector{var&quot;#s36&quot;} where var&quot;#s36&quot;&lt;:Real}"><code>elbo</code></a> for more details.</p><p><strong>Extended help</strong></p><p><code>q(f)</code> is assumed to be an <code>MvNormal</code> distribution and <code>p(y | f)</code> is assumed to have independent marginals such that only the marginals of <code>q(f)</code> are required.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/expected_loglik.jl#L17-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.expected_loglik-Tuple{DefaultQuadrature, AbstractVector{T} where T, AbstractVector{var&quot;#s8&quot;} where var&quot;#s8&quot;&lt;:Distributions.Normal, Any}" href="#ApproximateGPs.expected_loglik-Tuple{DefaultQuadrature, AbstractVector{T} where T, AbstractVector{var&quot;#s8&quot;} where var&quot;#s8&quot;&lt;:Distributions.Normal, Any}"><code>ApproximateGPs.expected_loglik</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">expected_loglik(::DefaultQuadrature, y::AbstractVector, q_f::AbstractVector{&lt;:Normal}, lik)</code></pre><p>The expected log likelihood. Defaults to a closed form solution if it exists, otherwise defaults to Gauss-Hermite quadrature.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/expected_loglik.jl#L44-L50">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.laplace_f_and_lml-Tuple{AbstractGPs.LatentFiniteGP, Any}" href="#ApproximateGPs.laplace_f_and_lml-Tuple{AbstractGPs.LatentFiniteGP, Any}"><code>ApproximateGPs.laplace_f_and_lml</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">laplace_f_and_lml(lfx::LatentFiniteGP, ys; newton_kwargs...)</code></pre><p>Compute a mode of the posterior and the Laplace approximation to the log marginal likelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L101-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.laplace_f_cov-Tuple{Any}" href="#ApproximateGPs.laplace_f_cov-Tuple{Any}"><code>ApproximateGPs.laplace_f_cov</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">laplace_f_cov(cache)</code></pre><p>Compute the covariance of <code>q(f)</code> from the results of the training computation that are stored in a <code>LaplaceCache</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L327-L332">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.laplace_lml-Tuple{AbstractGPs.LatentFiniteGP, Any}" href="#ApproximateGPs.laplace_lml-Tuple{AbstractGPs.LatentFiniteGP, Any}"><code>ApproximateGPs.laplace_lml</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">laplace_lml(lfx::LatentFiniteGP, ys; newton_kwargs...)</code></pre><p>Compute the Laplace approximation to the log marginal likelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L114-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.laplace_steps-Tuple{AbstractGPs.LatentFiniteGP, Any}" href="#ApproximateGPs.laplace_steps-Tuple{AbstractGPs.LatentFiniteGP, Any}"><code>ApproximateGPs.laplace_steps</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">laplace_steps(lfx::LatentFiniteGP, ys; newton_kwargs...)</code></pre><p>For demonstration purposes: returns an array of all the intermediate approximations of each Newton step.</p><p>If you are only interested in the actual posterior, use <code>posterior(::LaplaceApproximation, ...</code>.</p><p>TODO figure out how to get the <code>@ref</code> to work to point to the LaplaceApproximation-specific <code>posterior</code> docstring...</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L354-L364">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.loglik_and_derivs-Tuple{Any, AbstractVector{T} where T, AbstractVector{var&quot;#s37&quot;} where var&quot;#s37&quot;&lt;:Real}" href="#ApproximateGPs.loglik_and_derivs-Tuple{Any, AbstractVector{T} where T, AbstractVector{var&quot;#s37&quot;} where var&quot;#s37&quot;&lt;:Real}"><code>ApproximateGPs.loglik_and_derivs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">loglik_and_derivs(dist_y_given_f, ys, f)</code></pre><p><code>dist_y_given_f</code> must be a scalar function from a Real to a Distribution object. <code>ys</code> and <code>f</code> are vectors of observations and latent function values, respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L184-L189">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ApproximateGPs.newton_inner_loop-Tuple{Any, Any, Any}" href="#ApproximateGPs.newton_inner_loop-Tuple{Any, Any, Any}"><code>ApproximateGPs.newton_inner_loop</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">newton_inner_loop(dist_y_given_f, ys, K; f_init, maxiter, callback=nothing)</code></pre><p>Find a mode of <code>p(f | y)</code> using Newton&#39;s method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGaussianProcesses/ApproximateGPs.jl/blob/4d207c5078c18ed9a846ab9426fc7d27b1639a70/src/laplace.jl#L259-L263">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../userguide/">« User Guide</a><a class="docs-footer-nextpage" href="../examples/a-regression/">Regression: Sparse Variational Gaussian Process for Stochastic Optimisation with Flux.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Friday 5 November 2021 14:05">Friday 5 November 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
